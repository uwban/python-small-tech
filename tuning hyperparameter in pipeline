#tuned_parameters = {
#    'vect__max_df': (0.5, 0.75, 1.0),
#    'vect__max_features': (None, 5000, 10000, 50000),
#    'vect__ngram_range': ((1, 1), (1, 2)),  # unigrams or bigrams
#    'tfidf__use_idf': (True, False),
#    'tfidf__norm': ('l1', 'l2'),
#    'clf__max_iter': (5,),
#    'clf__alpha': (0.00001, 0.000001),
#    'clf__penalty': ('l2', 'elasticnet'),
#    'clf__max_iter': (10, 50, 80),
#}
#scores = ['precision', 'recall']
#pipeline = Pipeline([('vect', count),('tfidf', TfidfTransformer()),('clf', clf),])
for score in scores:
    print("# Tuning hyper-parameters for %s" % score)
    print()

    aclf = GridSearchCV(pipeline, tuned_parameters, cv=5,
                       scoring='%s_macro' % score)
    aclf.fit(X_train, y_train)

    print("Best parameters set found on development set:")
    print()
    print(aclf.best_params_)
    print()
    print("Grid scores on development set:")
    print()
    means = aclf.cv_results_['mean_test_score']
    stds = aclf.cv_results_['std_test_score']
    for mean, std, params in zip(means, stds, aclf.cv_results_['params']):
        print("%0.3f (+/-%0.03f) for %r"
              % (mean, std * 2, params))
    print()

    print("Detailed classification report:")
    print()
    print("The model is trained on the full development set.")
    print("The scores are computed on the full evaluation set.")
    print()
    y_pred = aclf.predict(X_test)
    print(classification_report(y_true, y_pred))
    print()
